{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7e09b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text==2.8.* in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (2.8.2)\n",
      "Requirement already satisfied: tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\" in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow-text==2.8.*) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.44.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (2.0)\n",
      "Requirement already satisfied: setuptools in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (44.0.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (3.20.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (0.5.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (13.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (4.1.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.16.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.22.3)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (2.8.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (2.8.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (3.6.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (0.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (2.27.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (2.6.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (3.3.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.26.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (4.11.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0; platform_machine != \"arm64\" or platform_system != \"Darwin\"->tensorflow-text==2.8.*) (3.7.0)\n",
      "Requirement already satisfied: bokeh in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (2.4.2)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from bokeh) (9.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from bokeh) (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from bokeh) (1.22.3)\n",
      "Requirement already satisfied: tornado>=5.1 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from bokeh) (6.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from bokeh) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from Jinja2>=2.9->bokeh) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from packaging>=16.8->bokeh) (3.0.7)\n",
      "Requirement already satisfied: simpleneighbors[annoy] in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (0.1.0)\n",
      "Requirement already satisfied: annoy>=1.16.0; extra == \"annoy\" in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (from simpleneighbors[annoy]) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /home/edwinjose/Documents/nlp/lib/python3.8/site-packages (4.64.0)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "#@title Setup Environment\n",
    "# Install the latest Tensorflow version.\n",
    "!pip install \"tensorflow-text==2.8.*\"\n",
    "!pip install bokeh\n",
    "!pip install simpleneighbors[annoy]\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1752583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61cc41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86dd6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 12:17:36.394925: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-25 12:17:36.395003: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import codecs\n",
    "import pandas as pd\n",
    "tqdm.pandas()\n",
    "import string\n",
    "from multiprocessing import  Pool\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import bokeh\n",
    "import bokeh.models\n",
    "import bokeh.plotting\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "import sklearn.metrics.pairwise\n",
    "\n",
    "from simpleneighbors import SimpleNeighbors\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239b6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parallelize_dataframe(df, func, n_cores=2):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ed174c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__pycache__',\n",
       " 'NLP Full.ipynb',\n",
       " 'dummy',\n",
       " 'Post_processing and Classiffication.ipynb',\n",
       " 'embedder.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'splitData']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859aebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./splitData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e661346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not_censored.csv', 'censored.csv', '.~lock.censored.csv#']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3407cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored=pd.read_csv(path+\"/censored.csv\",names=[\"mid\",\"text\",\"created_at\",\"deleted_last_seen\",\"permission_denied\"\n",
    "]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78f582ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored = df_censored.drop(df_censored.index[df_censored['text'] == \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aac7181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38965"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_censored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea4a19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"mid\",\"text\",\"created_at\",\"deleted_last_seen\",\"permission_denied\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b92719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c01ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncensored=pd.read_csv(path+\"/not_censored.csv\",names=[\"mid\",\"text\",\"created_at\",\"deleted_last_seen\",\"permission_denied\"\n",
    "],nrows=38965*2).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7cddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a989e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 12:17:50.991976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-25 12:17:50.992550: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-25 12:17:50.992694: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-25 12:17:50.992814: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-25 12:17:50.992932: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-25 12:17:50.993049: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-25 12:17:50.993166: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-25 12:17:50.993282: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-25 12:17:50.993398: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-25 12:17:50.993416: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-25 12:17:50.994183: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# The 16-language multilingual module is the default but feel free\n",
    "# to pick others from the list and compare the results.\n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3' #@param ['https://tfhub.dev/google/universal-sentence-encoder-multilingual/3', 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3']\n",
    "\n",
    "model = hub.load(module_url)\n",
    "\n",
    "def embed_text(input):\n",
    "    return model(input)[-1]\n",
    "\n",
    "def embed_text_list(input):\n",
    "    return model(input).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "902580d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_guse(df):\n",
    "    #df[\"guse\"] = df[\"text\"].progress_apply(lambda x:embed_text(str(x)))\n",
    "    df[\"guse\"] = embed_text_list(df[\"text\"].to_list()).tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbca89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test=parallelize_dataframe(df=df_censored[:200], func=parallel_guse, n_cores=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc78c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel_guse(df_censored[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df39cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_dataframe(df, func, n_splits=2):\n",
    "    df_split = np.array_split(df, n_splits)\n",
    "    df = pd.DataFrame()\n",
    "    for df_s in tqdm(df_split):\n",
    "        df_rs=func(df_s)\n",
    "        df = pd.concat([df,df_s])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88b335a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [01:23<00:00,  4.18s/it]\n"
     ]
    }
   ],
   "source": [
    "df_censored=serial_dataframe(df=df_censored, func=parallel_guse, n_splits=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6add7aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [02:28<00:00,  3.72s/it]\n"
     ]
    }
   ],
   "source": [
    "df_uncensored=serial_dataframe(df=df_uncensored, func=parallel_guse, n_splits=20*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ded33eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>deleted_last_seen</th>\n",
       "      <th>permission_denied</th>\n",
       "      <th>guse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mCClUNCqwe</td>\n",
       "      <td>转发微博</td>\n",
       "      <td>2012-01-03 02:02:27</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.10424910485744476, 0.044644154608249664, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mRsOcOLTlc</td>\n",
       "      <td>uK3RXUYW3： u0AGMTTVD：  ！！！！！！！！</td>\n",
       "      <td>2012-01-03 01:17:39</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.004717719741165638, -0.06476743519306183, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mH44qG6iUm</td>\n",
       "      <td>求一切順利</td>\n",
       "      <td>2012-01-03 01:15:36</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0962323397397995, -0.0058897421695292, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mZmwFtOdVX</td>\n",
       "      <td>想要uK3RXUYW3： ukn：  全都想要啊QAQ</td>\n",
       "      <td>2012-01-03 01:12:55</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.00019055344455409795, -0.026424074545502663...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mQkLJSl8bf</td>\n",
       "      <td>ukn：  uMLLV3ZCO：  转发微博</td>\n",
       "      <td>2012-01-03 01:10:42</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.015181104652583599, 0.01389026828110218, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mid                             text           created_at  \\\n",
       "0  mCClUNCqwe                             转发微博  2012-01-03 02:02:27   \n",
       "1  mRsOcOLTlc  uK3RXUYW3： u0AGMTTVD：  ！！！！！！！！  2012-01-03 01:17:39   \n",
       "2  mH44qG6iUm                            求一切順利  2012-01-03 01:15:36   \n",
       "3  mZmwFtOdVX      想要uK3RXUYW3： ukn：  全都想要啊QAQ  2012-01-03 01:12:55   \n",
       "4  mQkLJSl8bf           ukn：  uMLLV3ZCO：  转发微博  2012-01-03 01:10:42   \n",
       "\n",
       "  deleted_last_seen  permission_denied  \\\n",
       "0             False              False   \n",
       "1             False              False   \n",
       "2             False              False   \n",
       "3             False              False   \n",
       "4             False              False   \n",
       "\n",
       "                                                guse  \n",
       "0  [0.10424910485744476, 0.044644154608249664, 0....  \n",
       "1  [-0.004717719741165638, -0.06476743519306183, ...  \n",
       "2  [0.0962323397397995, -0.0058897421695292, 0.02...  \n",
       "3  [0.00019055344455409795, -0.026424074545502663...  \n",
       "4  [0.015181104652583599, 0.01389026828110218, 0....  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uncensored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bec55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=pd.concat([df_censored.loc[:,['permission_denied','guse']],df_uncensored.loc[:,['permission_denied','guse']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f20df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[\"permission_denied\"]=df_data[\"permission_denied\"].astype(bool).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e80c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_data[\"guse\"].to_numpy()\n",
    "X=np.vstack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8d39a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df_data[\"permission_denied\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5bc2f90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116725,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5ec7c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27406d66",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/edwinjose/Documents/nlp/lib/python3.8/site-packages/scikit_learn-1.0.2.dist-info/METADATA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:3021\u001b[0m, in \u001b[0;36mDistInfoDistribution._dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3020\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__dep_map\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:2815\u001b[0m, in \u001b[0;36mDistribution.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 2815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr)\n\u001b[1;32m   2816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_provider, attr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: _DistInfoDistribution__dep_map",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:3012\u001b[0m, in \u001b[0;36mDistInfoDistribution._parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3011\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pkg_info\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:2815\u001b[0m, in \u001b[0;36mDistribution.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 2815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr)\n\u001b[1;32m   2816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_provider, attr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: _pkg_info",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautosklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/autosklearn/__init__.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m requirements \u001b[38;5;241m=\u001b[39m pkg_resources\u001b[38;5;241m.\u001b[39mresource_string(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautosklearn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequirements.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m requirements \u001b[38;5;241m=\u001b[39m requirements\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mdependencies\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_packages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposix\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDetected unsupported operating system: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Please check \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe compability information of auto-sklearn: https://automl.github.io\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/auto-sklearn/stable/installation.html#windows-osx-compability\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m     20\u001b[0m         sys\u001b[38;5;241m.\u001b[39mplatform\n\u001b[1;32m     21\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/autosklearn/util/dependencies.py:25\u001b[0m, in \u001b[0;36mverify_packages\u001b[0;34m(packages)\u001b[0m\n\u001b[1;32m     23\u001b[0m     operation \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moperation1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m     version \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     \u001b[43m_verify_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to read requirement: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m package)\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/autosklearn/util/dependencies.py:34\u001b[0m, in \u001b[0;36m_verify_package\u001b[0;34m(name, operation, version)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_verify_package\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, operation: Optional[\u001b[38;5;28mstr\u001b[39m], version: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m         module_dist \u001b[38;5;241m=\u001b[39m \u001b[43mpkg_resources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m         installed_version \u001b[38;5;241m=\u001b[39m LooseVersion(module_dist\u001b[38;5;241m.\u001b[39mversion)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pkg_resources\u001b[38;5;241m.\u001b[39mDistributionNotFound:\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:481\u001b[0m, in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    479\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Requirement\u001b[38;5;241m.\u001b[39mparse(dist)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Requirement):\n\u001b[0;32m--> 481\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mget_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Distribution):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected string, Requirement, or Distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m, dist)\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:357\u001b[0m, in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(moduleOrReq, Requirement):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m working_set\u001b[38;5;241m.\u001b[39mfind(moduleOrReq) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mrequire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmoduleOrReq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[moduleOrReq]\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:900\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequire\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mrequirements):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;124;03m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \n\u001b[1;32m    894\u001b[0m \u001b[38;5;124;03m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m     needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_requirements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m needed:\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(dist)\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:794\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VersionConflict(dist, req)\u001b[38;5;241m.\u001b[39mwith_context(dependent_req)\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# push the new requirements onto the stack\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m new_requirements \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextras\u001b[49m\u001b[43m)\u001b[49m[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    795\u001b[0m requirements\u001b[38;5;241m.\u001b[39mextend(new_requirements)\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Register the new requirements needed by req\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:2736\u001b[0m, in \u001b[0;36mDistribution.requires\u001b[0;34m(self, extras)\u001b[0m\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires\u001b[39m(\u001b[38;5;28mself\u001b[39m, extras\u001b[38;5;241m=\u001b[39m()):\n\u001b[1;32m   2735\u001b[0m     \u001b[38;5;124;03m\"\"\"List of Requirements needed for this distro if `extras` are used\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2736\u001b[0m     dm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dep_map\u001b[49m\n\u001b[1;32m   2737\u001b[0m     deps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2738\u001b[0m     deps\u001b[38;5;241m.\u001b[39mextend(dm\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28;01mNone\u001b[39;00m, ()))\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:3023\u001b[0m, in \u001b[0;36mDistInfoDistribution._dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__dep_map\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 3023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__dep_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__dep_map\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:3032\u001b[0m, in \u001b[0;36mDistInfoDistribution._compute_dependencies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3030\u001b[0m reqs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3031\u001b[0m \u001b[38;5;66;03m# Including any condition expressions\u001b[39;00m\n\u001b[0;32m-> 3032\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m req \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parsed_pkg_info\u001b[49m\u001b[38;5;241m.\u001b[39mget_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequires-Dist\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[1;32m   3033\u001b[0m     reqs\u001b[38;5;241m.\u001b[39mextend(parse_requirements(req))\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreqs_for_extra\u001b[39m(extra):\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:3014\u001b[0m, in \u001b[0;36mDistInfoDistribution._parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pkg_info\n\u001b[1;32m   3013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 3014\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPKG_INFO\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pkg_info \u001b[38;5;241m=\u001b[39m email\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mParser()\u001b[38;5;241m.\u001b[39mparsestr(metadata)\n\u001b[1;32m   3016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pkg_info\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:1420\u001b[0m, in \u001b[0;36mNullProvider.get_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1419\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_path(name)\n\u001b[0;32m-> 1420\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY2:\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/pkg_resources/__init__.py:1616\u001b[0m, in \u001b[0;36mDefaultProvider._get\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m-> 1616\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m   1617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/edwinjose/Documents/nlp/lib/python3.8/site-packages/scikit_learn-1.0.2.dist-info/METADATA'"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# import sklearn.datasets\n",
    "# import sklearn.metrics\n",
    "\n",
    "# import autosklearn.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60623e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef8bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1f72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d551b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: hpsklearn 0.1.0\r\n",
      "Uninstalling hpsklearn-0.1.0:\r\n",
      "  Successfully uninstalled hpsklearn-0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall hpsklearn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae0f95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall hyperopt==0.2.5 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2feadb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hpsklearn import HyperoptEstimator, any_classifier\n",
    "# # from sklearn.datasets import fetch_mldata\n",
    "# from hyperopt import tpe\n",
    "# import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be056b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                      | 0/1 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.random.mtrand.RandomState' object has no attribute 'integers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y[ indices[\u001b[38;5;241m-\u001b[39mtest_size:]]\n\u001b[1;32m     17\u001b[0m estim \u001b[38;5;241m=\u001b[39m HyperoptEstimator( classifier\u001b[38;5;241m=\u001b[39many_classifier(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m),  \n\u001b[1;32m     18\u001b[0m                             algo\u001b[38;5;241m=\u001b[39mtpe\u001b[38;5;241m.\u001b[39msuggest, trial_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mestim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m( estim\u001b[38;5;241m.\u001b[39mscore( X_test, y_test ) )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# <<show score here>>\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/hpsklearn/estimator.py:746\u001b[0m, in \u001b[0;36mhyperopt_estimator.fit\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, cv_shuffle, warm_start, random_state, weights)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m<\u001b[39m adjusted_max_evals:\n\u001b[1;32m    744\u001b[0m     increment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_increment,\n\u001b[1;32m    745\u001b[0m                     adjusted_max_evals \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mtrials))\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mfit_iter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dump_file:\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/hpsklearn/estimator.py:648\u001b[0m, in \u001b[0;36mhyperopt_estimator.fit_iter\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, cv_shuffle, warm_start, random_state, weights, increment)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m#FIXME: temporary workaround for rstate issue #35\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m#       latest hyperopt.fmin() on master does not match PyPI\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrstate\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mgetargspec(hyperopt\u001b[38;5;241m.\u001b[39mfmin)\u001b[38;5;241m.\u001b[39margs:\n\u001b[0;32m--> 648\u001b[0m     \u001b[43mhyperopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn_with_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m                  \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# -- let exceptions crash the program,\u001b[39;49;00m\n\u001b[1;32m    655\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m#    so we notice them.\u001b[39;49;00m\n\u001b[1;32m    656\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# -- in case no success so far\u001b[39;49;00m\n\u001b[1;32m    658\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/nlp/lib/python3.8/site-packages/hyperopt/fmin.py:279\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Based on existing trials and the domain, use `algo` to probe in\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# new hp points. Save the results of those inspections into\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# `new_trials`. This is the core of `run`, all the rest is just\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# processes orchestration\u001b[39;00m\n\u001b[1;32m    278\u001b[0m new_trials \u001b[38;5;241m=\u001b[39m algo(\n\u001b[0;32m--> 279\u001b[0m     new_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain, trials, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegers\u001b[49m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    280\u001b[0m )\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_ids) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_trials)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_trials):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.random.mtrand.RandomState' object has no attribute 'integers'"
     ]
    }
   ],
   "source": [
    "\n",
    "# estim = HyperoptEstimator( classifier=any_classifier('clf'),  \n",
    "#                             algo=tpe.suggest, trial_timeout=300)\n",
    "\n",
    "\n",
    "# X=X\n",
    "# y=Y\n",
    "\n",
    "# test_size = int( 0.2 * len( y ) )\n",
    "# # np.random.seed( seed )\n",
    "# # np.random.RandomState(2)\n",
    "# indices = np.random.permutation(len(X))\n",
    "# X_train = X[ indices[:-test_size]]\n",
    "# y_train = y[ indices[:-test_size]]\n",
    "# X_test = X[ indices[-test_size:]]\n",
    "# y_test = y[ indices[-test_size:]]\n",
    "\n",
    "# estim = HyperoptEstimator( classifier=any_classifier('clf'),  \n",
    "#                             algo=tpe.suggest, trial_timeout=300)\n",
    "\n",
    "# estim.fit( X_train, y_train )\n",
    "\n",
    "# print( estim.score( X_test, y_test ) )\n",
    "# # <<show score here>>\n",
    "# print( estim.best_model() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31270fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
